{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T07:15:28.855759Z",
     "iopub.status.busy": "2025-04-14T07:15:28.855497Z",
     "iopub.status.idle": "2025-04-14T07:16:39.019602Z",
     "shell.execute_reply": "2025-04-14T07:16:39.018984Z",
     "shell.execute_reply.started": "2025-04-14T07:15:28.855740Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deep-translator\n",
      "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (4.13.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.1.31)\n",
      "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: deep-translator\n",
      "Successfully installed deep-translator-1.11.4\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=2c0ce3e850f30a4163b3697e40d285fe733fcb2127d03c40bfeb58b5d0effe62\n",
      "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy<2,>=1.20 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.20->matplotlib) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.20->matplotlib) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.20->matplotlib) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.20->matplotlib) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.20->matplotlib) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.20->matplotlib) (2.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2,>=1.20->matplotlib) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2,>=1.20->matplotlib) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1.20->matplotlib) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2,>=1.20->matplotlib) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2,>=1.20->matplotlib) (2024.2.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.7.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.17->seaborn) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.17->seaborn) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.17->seaborn) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.17->seaborn) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.17->seaborn) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.17->seaborn) (2.4.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy!=1.24.0,>=1.17->seaborn) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy!=1.24.0,>=1.17->seaborn) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy!=1.24.0,>=1.17->seaborn) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy!=1.24.0,>=1.17->seaborn) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy!=1.24.0,>=1.17->seaborn) (2024.2.0)\n",
      "Collecting ru-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.7.0/ru_core_news_sm-3.7.0-py3-none-any.whl (15.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from ru-core-news-sm==3.7.0) (3.7.5)\n",
      "Collecting pymorphy3>=1.0.0 (from ru-core-news-sm==3.7.0)\n",
      "  Downloading pymorphy3-2.0.3-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting dawg2-python>=0.8.0 (from pymorphy3>=1.0.0->ru-core-news-sm==3.7.0)\n",
      "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-sm==3.7.0)\n",
      "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.11.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.3.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2025.1.31)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.19.1)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.17.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.1.2)\n",
      "Downloading pymorphy3-2.0.3-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
      "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3, ru-core-news-sm\n",
      "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.3 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-sm-3.7.0\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ru_core_news_sm')\n",
      "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "Collecting uk-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/uk_core_news_sm-3.7.0/uk_core_news_sm-3.7.0-py3-none-any.whl (14.9 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from uk-core-news-sm==3.7.0) (3.7.5)\n",
      "Requirement already satisfied: pymorphy3>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from uk-core-news-sm==3.7.0) (2.0.3)\n",
      "Collecting pymorphy3-dicts-uk (from uk-core-news-sm==3.7.0)\n",
      "  Downloading pymorphy3_dicts_uk-2.4.1.1.1663094765-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from pymorphy3>=1.0.0->uk-core-news-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.11/dist-packages (from pymorphy3>=1.0.0->uk-core-news-sm==3.7.0) (2.4.417150.4580142)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (2.11.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (1.3.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (2.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (2025.1.31)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (2.19.1)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (1.17.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->uk-core-news-sm==3.7.0) (0.1.2)\n",
      "Downloading pymorphy3_dicts_uk-2.4.1.1.1663094765-py2.py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymorphy3-dicts-uk, uk-core-news-sm\n",
      "Successfully installed pymorphy3-dicts-uk-2.4.1.1.1663094765 uk-core-news-sm-3.7.0\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('uk_core_news_sm')\n",
      "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 07:16:27.746403: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744614987.977856      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744614988.052453      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install deep-translator\n",
    "!pip install langdetect  \n",
    "!pip install transformers\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!python -m spacy download ru_core_news_sm\n",
    "!python -m spacy download uk_core_news_sm\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import emoji\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from langdetect import detect\n",
    "from deep_translator import GoogleTranslator\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T07:16:52.321127Z",
     "iopub.status.busy": "2025-04-14T07:16:52.320585Z",
     "iopub.status.idle": "2025-04-14T07:16:55.366585Z",
     "shell.execute_reply": "2025-04-14T07:16:55.365754Z",
     "shell.execute_reply.started": "2025-04-14T07:16:52.321105Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import emoji\n",
    "import spacy\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, hamming_loss, accuracy_score\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import Dataset\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T07:17:04.709704Z",
     "iopub.status.busy": "2025-04-14T07:17:04.708846Z",
     "iopub.status.idle": "2025-04-14T07:17:05.042136Z",
     "shell.execute_reply": "2025-04-14T07:17:05.041517Z",
     "shell.execute_reply.started": "2025-04-14T07:17:04.709679Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add these imports to your existing imports\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T07:17:07.337996Z",
     "iopub.status.busy": "2025-04-14T07:17:07.337583Z",
     "iopub.status.idle": "2025-04-14T07:17:07.753382Z",
     "shell.execute_reply": "2025-04-14T07:17:07.752328Z",
     "shell.execute_reply.started": "2025-04-14T07:17:07.337964Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Shape: (3822, 6)\n",
      "Training Dataset Columns: Index(['id', 'content', 'lang', 'manipulative', 'techniques', 'trigger_words'], dtype='object')\n",
      "Training Dataset Head:\n",
      "                                      id  \\\n",
      "0  0bb0c7fa-101b-4583-a5f9-9d503339141c   \n",
      "1  7159f802-6f99-4e9d-97bd-6f565a4a0fae   \n",
      "2  e6a427f1-211f-405f-bd8b-70798458d656   \n",
      "3  1647a352-4cd3-40f6-bfa1-d87d42e34eea   \n",
      "4  9c01de00-841f-4b50-9407-104e9ffb03bf   \n",
      "\n",
      "                                             content lang  manipulative  \\\n",
      "0  –ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...   uk          True   \n",
      "1  –ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...   ru          True   \n",
      "2  ü§©\\n–¢–∏–º —á–∞—Å–æ–º –π–¥–µ –µ–≤–∞–∫—É–∞—Ü—ñ—è –ë—î–ª–≥–æ—Ä–æ–¥—Å—å–∫–æ–≥–æ –∞–≤—Ç–æ...   uk          True   \n",
      "3  –í –£–∫—Ä–∞—ó–Ω—ñ –Ω–∞–π–±–ª–∏–∂—á–∏–º —á–∞—Å–æ–º –º–∞—é—Ç—å –Ω–∞–º—ñ—Ä –ø–æ—Å–∏–ª–∏—Ç...   uk         False   \n",
      "4  –†–∞—Å—á—ë—Ç—ã 122-–º–º –°–ê–£ 2–°1 \"–ì–≤–æ–∑–¥–∏–∫–∞\" 132-–π –±—Ä–∏–≥–∞–¥...   ru          True   \n",
      "\n",
      "                             techniques  \\\n",
      "0        ['euphoria' 'loaded_language']   \n",
      "1  ['loaded_language' 'cherry_picking']   \n",
      "2        ['loaded_language' 'euphoria']   \n",
      "3                                   NaN   \n",
      "4                   ['loaded_language']   \n",
      "\n",
      "                                       trigger_words  \n",
      "0  [array([27, 63]) array([65, 88]) array([ 90, 1...  \n",
      "1  [array([ 0, 40]) array([123, 137]) array([180,...  \n",
      "2                                [array([ 55, 100])]  \n",
      "3                                                NaN  \n",
      "4                                [array([114, 144])]  \n",
      "Test Dataset Shape (raw): (5736, 2)\n",
      "Test Dataset Columns (raw): Index(['Column1', 'Column2'], dtype='object')\n",
      "Test Dataset Head (raw):\n",
      "                                 Column1  \\\n",
      "0                                    id   \n",
      "1  521cd2e8-dd9f-42c4-98ba-c0c8890ff1ba   \n",
      "2  9b2a61e4-d14e-4ff7-b304-e73d720319bf   \n",
      "3  f0f1c236-80a8-4d25-b30c-a420a39be632   \n",
      "4  31ea05ba-2c2b-4b84-aba7-f3cf6841b204   \n",
      "\n",
      "                                             Column2  \n",
      "0                                            content  \n",
      "1  –û–Ω–∏ –ø—Ä–æ—Å—Ä–∞–ª–∏ –Ω–∞—à—É —Ç–µ—Ö–Ω–∏–∫—É, –ø–æ–ª–æ–∂–∏–ª–∏ –∫—É—á—É –ª—é–¥–µ–π...  \n",
      "2  ‚ùóÔ∏è\\n–ö–∏—Ç–∞–π –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –æ—Ç–¥–∞—Ç—å –æ–∫–∫—É–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Ä...  \n",
      "3  –°–µ–≥–æ–¥–Ω—è –±—É–¥–µ—Ç —Ä–æ–≤–Ω–æ 6 –º–µ—Å—è—Ü–µ–≤ —Å —ç—Ç–æ–≥–æ –æ–±–µ—â–∞–Ω–∏—è...  \n",
      "4  ‚ö°Ô∏è\\n–Ü–∑—Ä–∞—ó–ª—å –≤–ø–µ—Ä—à–µ —É —Å–≤—ñ—Ç—ñ –∑–±–∏–≤ –±–∞–ª—ñ—Å—Ç–∏—á–Ω—É —Ä–∞–∫...  \n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_csv('/kaggle/input/finaldataset/NLP - exp(Encoded UTF-8).csv')\n",
    "print(\"Training Dataset Shape:\", train_df.shape)\n",
    "print(\"Training Dataset Columns:\", train_df.columns)\n",
    "print(\"Training Dataset Head:\\n\", train_df.head())\n",
    "\n",
    "# Load test dataset without skipping rows\n",
    "test_df = pd.read_csv('/kaggle/input/finaldataset/final test.csv')\n",
    "print(\"Test Dataset Shape (raw):\", test_df.shape)\n",
    "print(\"Test Dataset Columns (raw):\", test_df.columns)\n",
    "print(\"Test Dataset Head (raw):\\n\", test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T07:17:11.085188Z",
     "iopub.status.busy": "2025-04-14T07:17:11.084889Z",
     "iopub.status.idle": "2025-04-14T07:17:16.319292Z",
     "shell.execute_reply": "2025-04-14T07:17:16.318450Z",
     "shell.execute_reply.started": "2025-04-14T07:17:11.085168Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting test_df columns because 'content' is missing...\n",
      "Adjusted Test Dataset Columns: Index(['id', 'content'], dtype='object')\n",
      "Adjusted Test Dataset Head:\n",
      "                                      id  \\\n",
      "0                                    id   \n",
      "1  521cd2e8-dd9f-42c4-98ba-c0c8890ff1ba   \n",
      "2  9b2a61e4-d14e-4ff7-b304-e73d720319bf   \n",
      "3  f0f1c236-80a8-4d25-b30c-a420a39be632   \n",
      "4  31ea05ba-2c2b-4b84-aba7-f3cf6841b204   \n",
      "\n",
      "                                             content  \n",
      "0                                            content  \n",
      "1  –û–Ω–∏ –ø—Ä–æ—Å—Ä–∞–ª–∏ –Ω–∞—à—É —Ç–µ—Ö–Ω–∏–∫—É, –ø–æ–ª–æ–∂–∏–ª–∏ –∫—É—á—É –ª—é–¥–µ–π...  \n",
      "2  ‚ùóÔ∏è\\n–ö–∏—Ç–∞–π –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –æ—Ç–¥–∞—Ç—å –æ–∫–∫—É–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Ä...  \n",
      "3  –°–µ–≥–æ–¥–Ω—è –±—É–¥–µ—Ç —Ä–æ–≤–Ω–æ 6 –º–µ—Å—è—Ü–µ–≤ —Å —ç—Ç–æ–≥–æ –æ–±–µ—â–∞–Ω–∏—è...  \n",
      "4  ‚ö°Ô∏è\\n–Ü–∑—Ä–∞—ó–ª—å –≤–ø–µ—Ä—à–µ —É —Å–≤—ñ—Ç—ñ –∑–±–∏–≤ –±–∞–ª—ñ—Å—Ç–∏—á–Ω—É —Ä–∞–∫...  \n",
      "Initial Test Data Shape: (5736, 2)\n",
      "First few rows of test_df:\n",
      "                                      id  \\\n",
      "0                                    id   \n",
      "1  521cd2e8-dd9f-42c4-98ba-c0c8890ff1ba   \n",
      "2  9b2a61e4-d14e-4ff7-b304-e73d720319bf   \n",
      "3  f0f1c236-80a8-4d25-b30c-a420a39be632   \n",
      "4  31ea05ba-2c2b-4b84-aba7-f3cf6841b204   \n",
      "\n",
      "                                             content  \n",
      "0                                            content  \n",
      "1  –û–Ω–∏ –ø—Ä–æ—Å—Ä–∞–ª–∏ –Ω–∞—à—É —Ç–µ—Ö–Ω–∏–∫—É, –ø–æ–ª–æ–∂–∏–ª–∏ –∫—É—á—É –ª—é–¥–µ–π...  \n",
      "2  ‚ùóÔ∏è\\n–ö–∏—Ç–∞–π –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –æ—Ç–¥–∞—Ç—å –æ–∫–∫—É–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Ä...  \n",
      "3  –°–µ–≥–æ–¥–Ω—è –±—É–¥–µ—Ç —Ä–æ–≤–Ω–æ 6 –º–µ—Å—è—Ü–µ–≤ —Å —ç—Ç–æ–≥–æ –æ–±–µ—â–∞–Ω–∏—è...  \n",
      "4  ‚ö°Ô∏è\\n–Ü–∑—Ä–∞—ó–ª—å –≤–ø–µ—Ä—à–µ —É —Å–≤—ñ—Ç—ñ –∑–±–∏–≤ –±–∞–ª—ñ—Å—Ç–∏—á–Ω—É —Ä–∞–∫...  \n",
      "Test Data Shape after filtering: (5735, 2)\n",
      "Cleaned Train Dataset Head:\n",
      "                                              content\n",
      "0  –Ω–æ–≤–∏–∏ –æ–≥–ª—è–¥ –º–∞–ø–∏ deepstate –≤—ñ–¥ —Ä–æ—Å—ñ–∏—Å—å–∫–æ–≥–æ –≤—ñ–∏...\n",
      "1  –Ω–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂–µ—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...\n",
      "2  —Ç–∏–º —á–∞—Å–æ–º –∏–¥–µ –µ–≤–∞–∫—É–∞—Ü—ñ—è –±—î–ª–≥–æ—Ä–æ–¥—Å—å–∫–æ–≥–æ –∞–≤—Ç–æ–≤–æ–∫...\n",
      "3  –≤ —É–∫—Ä–∞—ñ–Ω—ñ –Ω–∞–∏–±–ª–∏–∂—á–∏–º —á–∞—Å–æ–º –º–∞—é—Ç—å –Ω–∞–º—ñ—Ä –ø–æ—Å–∏–ª–∏—Ç...\n",
      "4  —Ä–∞—Å—á–µ—Ç—ã 122–º–º —Å–∞—É 2—Å1 –≥–≤–æ–∑–¥–∏–∫–∞ 132–∏ –±—Ä–∏–≥–∞–¥—ã 1–≥...\n",
      "Cleaned Test Dataset Head:\n",
      "                                              content\n",
      "1  –æ–Ω–∏ –ø—Ä–æ—Å—Ä–∞–ª–∏ –Ω–∞—à—É —Ç–µ—Ö–Ω–∏–∫—É –ø–æ–ª–æ–∂–∏–ª–∏ –∫—É—á—É –ª—é–¥–µ–∏ ...\n",
      "2  –∫–∏—Ç–∞–∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –æ—Ç–¥–∞—Ç—å –æ–∫–∫—É–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Ä—Ä–∏—Ç–æ...\n",
      "3  —Å–µ–≥–æ–¥–Ω—è –±—É–¥–µ—Ç —Ä–æ–≤–Ω–æ 6 –º–µ—Å—è—Ü–µ–≤ —Å —ç—Ç–æ–≥–æ –æ–±–µ—â–∞–Ω–∏—è...\n",
      "4  —ñ–∑—Ä–∞—ñ–ª—å –≤–ø–µ—Ä—à–µ —É —Å–≤—ñ—Ç—ñ –∑–±–∏–≤ –±–∞–ª—ñ—Å—Ç–∏—á–Ω—É —Ä–∞–∫–µ—Ç—É ...\n",
      "5  —Å–∫–ª–∞–≤ –Ω–µ–≤–µ–ª–∏–∫—É –Ω–∞–≤—á–∞–ª—å–Ω–æ–º–µ—Ç–æ–¥–∏—á–Ω—É —Ç–∞–±–ª–∏—Ü—é –Ω–∞ —Ç...\n"
     ]
    }
   ],
   "source": [
    "# Check and fix column names if 'content' is missing\n",
    "if 'content' not in train_df.columns:\n",
    "    print(\"Error: 'content' not found in train_df. Please check the CSV file structure.\")\n",
    "    raise KeyError(\"'content' column missing in train_df\")\n",
    "\n",
    "if 'content' not in test_df.columns:\n",
    "    print(\"Adjusting test_df columns because 'content' is missing...\")\n",
    "    # Assume the second column is 'content' if it exists\n",
    "    if len(test_df.columns) >= 2:\n",
    "        test_df.columns = ['id', 'content'] \n",
    "    else:\n",
    "        print(\"Error: test_df has insufficient columns. Expected at least 'id' and 'content'.\")\n",
    "        raise KeyError(\"'content' column missing or test_df malformed\")\n",
    "    print(\"Adjusted Test Dataset Columns:\", test_df.columns)\n",
    "    print(\"Adjusted Test Dataset Head:\\n\", test_df.head())\n",
    "\n",
    "# Add step to filter test_df to exactly 5735 rows by removing invalid rows\n",
    "print(\"Initial Test Data Shape:\", test_df.shape)\n",
    "print(\"First few rows of test_df:\\n\", test_df.head())\n",
    "\n",
    "# Remove rows where the 'id' column is invalid (e.g., contains the string \"id\" or is not a proper data row)\n",
    "# This assumes the invalid row has 'id' as the value in the 'id' column, which is likely a duplicate header\n",
    "test_df = test_df[test_df['id'] != 'id']\n",
    "\n",
    "# If there are still more than 5735 rows, keep only the first 5735 valid rows\n",
    "# This ensures we don't accidentally remove too many rows if there are other invalid entries\n",
    "if len(test_df) > 5735:\n",
    "    test_df = test_df.iloc[:5735]\n",
    "elif len(test_df) < 5735:\n",
    "    raise ValueError(f\"Test data has fewer than 5735 rows after filtering ({len(test_df)} rows). Check test.csv file.\")\n",
    "\n",
    "print(\"Test Data Shape after filtering:\", test_df.shape)\n",
    "\n",
    "# Verify the row count\n",
    "if len(test_df) != 5735:\n",
    "    raise ValueError(f\"Test data should have 5735 rows, but found {len(test_df)} rows after filtering. Check test.csv file.\")\n",
    "    \n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = \"\".join([c for c in text if not unicodedata.combining(c)])\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-z–∞-—è—ë—ñ—ó—î“ë0-9\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "train_df[\"content\"] = train_df[\"content\"].apply(clean_text)\n",
    "test_df[\"content\"] = test_df[\"content\"].apply(clean_text)\n",
    "print(\"Cleaned Train Dataset Head:\\n\", train_df[[\"content\"]].head())\n",
    "print(\"Cleaned Test Dataset Head:\\n\", test_df[[\"content\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T07:17:21.248273Z",
     "iopub.status.busy": "2025-04-14T07:17:21.248006Z",
     "iopub.status.idle": "2025-04-14T07:17:21.267213Z",
     "shell.execute_reply": "2025-04-14T07:17:21.266486Z",
     "shell.execute_reply.started": "2025-04-14T07:17:21.248253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define manipulation techniques\n",
    "techniques = [\n",
    "    \"loaded_language\", \"glittering_generalities\", \"euphoria\", \"appeal_to_fear\", \"fud\",\n",
    "    \"bandwagon\", \"thought_terminating_cliche\", \"whataboutism\", \"cherry_picking\", \"straw_man\"\n",
    "]\n",
    "num_classes = len(techniques)\n",
    "\n",
    "# Convert techniques column to multilabel binary format\n",
    "def techniques_to_binary(tech_str):\n",
    "    if pd.isna(tech_str):\n",
    "        return [0] * num_classes\n",
    "    tech_str = tech_str.strip(\"[]\").replace(\"'\", \"\").replace('\"', '')\n",
    "    tech_list = [tech.strip() for tech in tech_str.split() if tech.strip()]\n",
    "    if not tech_list:\n",
    "        tech_list = [tech.strip() for tech in tech_str.split(',') if tech.strip()]\n",
    "    return [1 if tech in tech_list else 0 for tech in techniques]\n",
    "\n",
    "train_df[\"labels\"] = train_df[\"techniques\"].apply(techniques_to_binary)\n",
    "\n",
    "# Split dataset\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T07:17:23.591439Z",
     "iopub.status.busy": "2025-04-14T07:17:23.591155Z",
     "iopub.status.idle": "2025-04-14T07:17:27.127127Z",
     "shell.execute_reply": "2025-04-14T07:17:27.126118Z",
     "shell.execute_reply.started": "2025-04-14T07:17:23.591418Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nlpaug\n",
      "  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.32.3)\n",
      "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.13.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (3.18.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.67.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (4.13.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.2->nlpaug) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.16.2->nlpaug) (2024.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\n",
      "Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nlpaug\n",
      "Successfully installed nlpaug-1.1.11\n"
     ]
    }
   ],
   "source": [
    "!pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T07:17:36.155688Z",
     "iopub.status.busy": "2025-04-14T07:17:36.154834Z",
     "iopub.status.idle": "2025-04-14T07:18:23.529196Z",
     "shell.execute_reply": "2025-04-14T07:18:23.528523Z",
     "shell.execute_reply.started": "2025-04-14T07:17:36.155644Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting improved data augmentation for minority classes...\n",
      "Original class counts: {'loaded_language': 1581, 'glittering_generalities': 386, 'euphoria': 385, 'appeal_to_fear': 242, 'fud': 310, 'bandwagon': 122, 'thought_terminating_cliche': 0, 'whataboutism': 124, 'cherry_picking': 415, 'straw_man': 113}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2edfbeef0e5e47b995d59f16b77eb62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55bdebc8d9ca4288989872c6a1c0a814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5591ee47e75d4b3da0713e403a658e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f45e1a01324948adf15ca001e0ea32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ff34b732064dafb46a64aff02cea19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting glittering_generalities: Current=386, Target=500, Generating 114 new samples\n",
      "Augmenting euphoria: Current=385, Target=500, Generating 115 new samples\n",
      "Augmenting appeal_to_fear: Current=242, Target=500, Generating 258 new samples\n",
      "Augmenting fud: Current=310, Target=500, Generating 190 new samples\n",
      "Augmenting bandwagon: Current=122, Target=500, Generating 378 new samples\n",
      "Skipping thought_terminating_cliche as it has 0 samples\n",
      "Augmenting whataboutism: Current=124, Target=500, Generating 376 new samples\n",
      "Augmenting cherry_picking: Current=415, Target=500, Generating 85 new samples\n",
      "Augmenting straw_man: Current=113, Target=500, Generating 387 new samples\n",
      "New class counts after augmentation: {'loaded_language': 3025, 'glittering_generalities': 718, 'euphoria': 609, 'appeal_to_fear': 737, 'fud': 887, 'bandwagon': 604, 'thought_terminating_cliche': 0, 'whataboutism': 598, 'cherry_picking': 1018, 'straw_man': 639}\n"
     ]
    }
   ],
   "source": [
    "from nlpaug.augmenter.word import SynonymAug, BackTranslationAug, ContextualWordEmbsAug\n",
    "import nlpaug.flow as naf\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def augment_minority_classes_improved(df, techniques, min_samples=500):\n",
    "    print(\"Starting improved data augmentation for minority classes...\")\n",
    "    original_rows = df.to_dict('records')\n",
    "    augmented_rows = []\n",
    "    labels_array = np.array([row['labels'] for row in original_rows])\n",
    "    class_counts = labels_array.sum(axis=0)\n",
    "    print(\"Original class counts:\", {techniques[i]: count for i, count in enumerate(class_counts)})\n",
    "    \n",
    "    # Setup augmenters - customize per language if needed\n",
    "    synonym_aug = SynonymAug(aug_src='wordnet')  # For English content\n",
    "    \n",
    "    # Optional: Add back-translation augmenter if you have internet access\n",
    "    # back_trans_aug = BackTranslationAug(\n",
    "    #    from_model_name='Helsinki-NLP/opus-mt-ru-en', \n",
    "    #    to_model_name='Helsinki-NLP/opus-mt-en-ru'\n",
    "    # )\n",
    "    \n",
    "    # Contextual word embeddings augmenter (works better for maintaining propaganda techniques)\n",
    "    contextual_aug = ContextualWordEmbsAug(\n",
    "        model_path='xlm-roberta-base',\n",
    "        action=\"substitute\", \n",
    "        aug_p=0.1  # Probability of changing each word\n",
    "    )\n",
    "    \n",
    "    # Create augmentation flow - combine multiple techniques\n",
    "    aug_flow = naf.Sometimes([\n",
    "        synonym_aug,  \n",
    "        contextual_aug,\n",
    "        # back_trans_aug  # Uncomment if available\n",
    "    ], aug_p=0.7)\n",
    "    \n",
    "    for class_idx, count in enumerate(class_counts):\n",
    "        if count == 0:\n",
    "            print(f\"Skipping {techniques[class_idx]} as it has 0 samples\")\n",
    "            continue\n",
    "            \n",
    "        if count < min_samples:\n",
    "            class_rows = [row for row in original_rows if row['labels'][class_idx] == 1]\n",
    "            needed = min_samples - count\n",
    "            print(f\"Augmenting {techniques[class_idx]}: Current={count}, Target={min_samples}, Generating {needed} new samples\")\n",
    "            \n",
    "            for _ in range(needed):\n",
    "                row = random.choice(class_rows)\n",
    "                new_row = row.copy()\n",
    "                \n",
    "                # Apply language-specific augmentation based on detected language\n",
    "                if row['lang'] in ['ru', 'uk']:\n",
    "                    new_row['content'] = contextual_aug.augment(row['content'])\n",
    "                else:\n",
    "                    new_row['content'] = aug_flow.augment(row['content'])\n",
    "                    \n",
    "                augmented_rows.append(new_row)\n",
    "    \n",
    "    combined_data = original_rows + augmented_rows\n",
    "    new_df = pd.DataFrame(combined_data)\n",
    "    new_labels_array = np.array(new_df[\"labels\"].tolist())\n",
    "    new_class_counts = new_labels_array.sum(axis=0)\n",
    "    print(\"New class counts after augmentation:\", {techniques[i]: count for i, count in enumerate(new_class_counts)})\n",
    "    return new_df\n",
    "# Apply augmentation\n",
    "balanced_train_df = augment_minority_classes_improved(train_df, techniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T07:18:48.663216Z",
     "iopub.status.busy": "2025-04-14T07:18:48.662502Z",
     "iopub.status.idle": "2025-04-14T07:18:48.856564Z",
     "shell.execute_reply": "2025-04-14T07:18:48.855672Z",
     "shell.execute_reply.started": "2025-04-14T07:18:48.663192Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {'loaded_language': 0.0, 'glittering_generalities': 0.01, 'euphoria': 0.02, 'appeal_to_fear': 0.01, 'fud': 0.01, 'bandwagon': 0.02, 'thought_terminating_cliche': 9.88, 'whataboutism': 0.02, 'cherry_picking': 0.01, 'straw_man': 0.02}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define device (add this before your function calls)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Calculate class weights using balanced dataset\n",
    "def calculate_class_weights(df, techniques):\n",
    "    labels_array = np.array(df[\"labels\"].tolist())\n",
    "    class_counts = labels_array.sum(axis=0)\n",
    "    class_counts = np.array([max(1, count) for count in class_counts])\n",
    "    weights = len(df) / (len(techniques) * class_counts)\n",
    "    weights = weights / weights.sum() * len(techniques)\n",
    "    print(\"Class weights:\", {techniques[i]: round(weight, 2) for i, weight in enumerate(weights)})\n",
    "    return torch.tensor(weights, dtype=torch.float).to(device)\n",
    "\n",
    "class_weights = calculate_class_weights(balanced_train_df, techniques)\n",
    "\n",
    "# Weighted BCE loss function\n",
    "def weighted_bce_loss(outputs, targets):\n",
    "    probs = torch.sigmoid(outputs)\n",
    "    loss = 0\n",
    "    for i in range(targets.size(1)):\n",
    "        weight = class_weights[i]\n",
    "        loss += weight * (-targets[:, i] * torch.log(probs[:, i] + 1e-7) -\n",
    "                         (1 - targets[:, i]) * torch.log(1 - probs[:, i] + 1e-7))\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T07:18:53.730343Z",
     "iopub.status.busy": "2025-04-14T07:18:53.730061Z",
     "iopub.status.idle": "2025-04-14T07:19:08.229162Z",
     "shell.execute_reply": "2025-04-14T07:19:08.228521Z",
     "shell.execute_reply.started": "2025-04-14T07:18:53.730321Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb4b6de6cf24ef7a406664e71d72f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899fe73b41fa4aaba225480b95091a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ee6ba0a7f747999b4368f0f6ee74d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ce6ea696c54736a9a2e22d1681cb58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60cc9c96dac04fa6ac83b502523d308c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 4960\n",
      "Validation dataset size: 765\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "# Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=128):\n",
    "        self.df = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.texts = self.df[\"content\"].values\n",
    "        self.labels = self.df[\"labels\"].values if \"labels\" in self.df.columns else [[0] * num_classes] * len(self.df)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = np.array(self.labels[idx], dtype=np.float32)\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Model and tokenizer setup\n",
    "model_name = \"xlm-roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_classes,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TextDataset(balanced_train_df, tokenizer)\n",
    "val_dataset = TextDataset(val_df, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# Verify setup\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T07:19:12.609182Z",
     "iopub.status.busy": "2025-04-14T07:19:12.608230Z",
     "iopub.status.idle": "2025-04-14T08:11:02.518263Z",
     "shell.execute_reply": "2025-04-14T08:11:02.517692Z",
     "shell.execute_reply.started": "2025-04-14T07:19:12.609138Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6 [Training]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 620/620 [08:06<00:00,  1.27it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96/96 [00:19<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n",
      "  Train Loss: 0.0008\n",
      "  Val Loss: 0.0004, Macro F1: 0.1314\n",
      "\n",
      "Classification Report for Validation Set:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "           loaded_language       0.68      0.67      0.67       392\n",
      "   glittering_generalities       0.74      0.36      0.49        97\n",
      "                  euphoria       0.54      0.09      0.16        77\n",
      "            appeal_to_fear       0.00      0.00      0.00        58\n",
      "                       fud       0.00      0.00      0.00        75\n",
      "                 bandwagon       0.00      0.00      0.00        35\n",
      "thought_terminating_cliche       0.00      0.00      0.00         0\n",
      "              whataboutism       0.00      0.00      0.00        34\n",
      "            cherry_picking       0.00      0.00      0.00        97\n",
      "                 straw_man       0.00      0.00      0.00        25\n",
      "\n",
      "                 micro avg       0.68      0.34      0.45       890\n",
      "                 macro avg       0.20      0.11      0.13       890\n",
      "              weighted avg       0.43      0.34      0.36       890\n",
      "               samples avg       0.39      0.25      0.29       890\n",
      "\n",
      "  New best model saved with Macro F1: 0.1314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/6 [Training]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 620/620 [08:11<00:00,  1.26it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96/96 [00:19<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0004, Macro F1: 0.1508\n",
      "\n",
      "Classification Report for Validation Set:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "           loaded_language       0.59      0.94      0.72       392\n",
      "   glittering_generalities       0.95      0.21      0.34        97\n",
      "                  euphoria       0.44      0.25      0.32        77\n",
      "            appeal_to_fear       0.00      0.00      0.00        58\n",
      "                       fud       0.00      0.00      0.00        75\n",
      "                 bandwagon       0.15      0.11      0.13        35\n",
      "thought_terminating_cliche       0.00      0.00      0.00         0\n",
      "              whataboutism       0.00      0.00      0.00        34\n",
      "            cherry_picking       0.00      0.00      0.00        97\n",
      "                 straw_man       0.00      0.00      0.00        25\n",
      "\n",
      "                 micro avg       0.57      0.46      0.51       890\n",
      "                 macro avg       0.21      0.15      0.15       890\n",
      "              weighted avg       0.41      0.46      0.39       890\n",
      "               samples avg       0.48      0.35      0.38       890\n",
      "\n",
      "  New best model saved with Macro F1: 0.1508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/6 [Training]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 620/620 [08:11<00:00,  1.26it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96/96 [00:19<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3:\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004, Macro F1: 0.2306\n",
      "\n",
      "Classification Report for Validation Set:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "           loaded_language       0.69      0.78      0.73       392\n",
      "   glittering_generalities       0.60      0.67      0.63        97\n",
      "                  euphoria       0.55      0.29      0.38        77\n",
      "            appeal_to_fear       0.33      0.02      0.03        58\n",
      "                       fud       0.53      0.11      0.18        75\n",
      "                 bandwagon       0.18      0.06      0.09        35\n",
      "thought_terminating_cliche       0.00      0.00      0.00         0\n",
      "              whataboutism       0.08      0.03      0.04        34\n",
      "            cherry_picking       0.67      0.02      0.04        97\n",
      "                 straw_man       0.43      0.12      0.19        25\n",
      "\n",
      "                 micro avg       0.63      0.46      0.53       890\n",
      "                 macro avg       0.41      0.21      0.23       890\n",
      "              weighted avg       0.58      0.46      0.45       890\n",
      "               samples avg       0.43      0.34      0.36       890\n",
      "\n",
      "  New best model saved with Macro F1: 0.2306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/6 [Training]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 620/620 [08:11<00:00,  1.26it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96/96 [00:19<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4:\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0005, Macro F1: 0.3021\n",
      "\n",
      "Classification Report for Validation Set:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "           loaded_language       0.71      0.73      0.72       392\n",
      "   glittering_generalities       0.50      0.59      0.54        97\n",
      "                  euphoria       0.52      0.34      0.41        77\n",
      "            appeal_to_fear       0.36      0.22      0.28        58\n",
      "                       fud       0.49      0.39      0.43        75\n",
      "                 bandwagon       0.11      0.11      0.11        35\n",
      "thought_terminating_cliche       0.00      0.00      0.00         0\n",
      "              whataboutism       0.00      0.00      0.00        34\n",
      "            cherry_picking       0.52      0.27      0.35        97\n",
      "                 straw_man       0.33      0.12      0.18        25\n",
      "\n",
      "                 micro avg       0.58      0.50      0.54       890\n",
      "                 macro avg       0.35      0.28      0.30       890\n",
      "              weighted avg       0.55      0.50      0.51       890\n",
      "               samples avg       0.37      0.34      0.33       890\n",
      "\n",
      "  New best model saved with Macro F1: 0.3021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/6 [Training]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 620/620 [08:11<00:00,  1.26it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96/96 [00:19<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5:\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0007, Macro F1: 0.3285\n",
      "\n",
      "Classification Report for Validation Set:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "           loaded_language       0.70      0.79      0.74       392\n",
      "   glittering_generalities       0.57      0.58      0.57        97\n",
      "                  euphoria       0.38      0.53      0.44        77\n",
      "            appeal_to_fear       0.21      0.47      0.29        58\n",
      "                       fud       0.34      0.75      0.46        75\n",
      "                 bandwagon       0.13      0.20      0.16        35\n",
      "thought_terminating_cliche       0.00      0.00      0.00         0\n",
      "              whataboutism       0.06      0.06      0.06        34\n",
      "            cherry_picking       0.36      0.41      0.38        97\n",
      "                 straw_man       0.20      0.16      0.18        25\n",
      "\n",
      "                 micro avg       0.46      0.61      0.53       890\n",
      "                 macro avg       0.29      0.39      0.33       890\n",
      "              weighted avg       0.50      0.61      0.54       890\n",
      "               samples avg       0.35      0.41      0.36       890\n",
      "\n",
      "  New best model saved with Macro F1: 0.3285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/6 [Training]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 620/620 [08:11<00:00,  1.26it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96/96 [00:19<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6:\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0007, Macro F1: 0.3078\n",
      "\n",
      "Classification Report for Validation Set:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "           loaded_language       0.71      0.74      0.73       392\n",
      "   glittering_generalities       0.54      0.65      0.59        97\n",
      "                  euphoria       0.48      0.31      0.38        77\n",
      "            appeal_to_fear       0.21      0.22      0.22        58\n",
      "                       fud       0.46      0.43      0.44        75\n",
      "                 bandwagon       0.08      0.06      0.07        35\n",
      "thought_terminating_cliche       0.00      0.00      0.00         0\n",
      "              whataboutism       0.00      0.00      0.00        34\n",
      "            cherry_picking       0.38      0.39      0.39        97\n",
      "                 straw_man       0.26      0.28      0.27        25\n",
      "\n",
      "                 micro avg       0.55      0.53      0.54       890\n",
      "                 macro avg       0.31      0.31      0.31       890\n",
      "              weighted avg       0.52      0.53      0.52       890\n",
      "               samples avg       0.38      0.36      0.35       890\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96/96 [00:19<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold for loaded_language: 0.5\n",
      "Best threshold for glittering_generalities: 0.5\n",
      "Best threshold for euphoria: 0.5\n",
      "Best threshold for appeal_to_fear: 0.5\n",
      "Best threshold for fud: 0.6\n",
      "Best threshold for bandwagon: 0.4\n",
      "Best threshold for thought_terminating_cliche: 0.1\n",
      "Best threshold for whataboutism: 0.2\n",
      "Best threshold for cherry_picking: 0.4\n",
      "Best threshold for straw_man: 0.30000000000000004\n",
      "Training dataset size: 4960\n",
      "Validation dataset size: 765\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.flow as naf\n",
    "import nltk\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# [Previous code up to DataLoader creation remains unchanged...]\n",
    "\n",
    "# Focal Loss to better handle class imbalance\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean', class_weights=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.class_weights = class_weights  # Optional: Add class weights\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.BCEWithLogitsLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        \n",
    "        if self.class_weights is not None:\n",
    "            F_loss = F_loss * self.class_weights.to(inputs.device)  # Apply weights if provided\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "\n",
    "def train_model(model, train_loader, val_loader, techniques, num_epochs=6, patience=2):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)  # Increased weight_decay\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=1, verbose=True)\n",
    "    criterion = FocalLoss(gamma=3, class_weights=class_weights)  # Increased gamma for harder examples\n",
    "    best_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\"):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float().cpu().numpy()\n",
    "            train_preds.extend(preds)\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Validation phase with threshold tuning capability\n",
    "        val_metrics = evaluate_model(model, val_loader, techniques)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "        print(f\"  Val Loss: {val_metrics['loss']:.4f}, Macro F1: {val_metrics['macro_f1']:.4f}\")\n",
    "        print(f\"\\nClassification Report for Validation Set:\")\n",
    "        print(val_metrics['report'])\n",
    "        \n",
    "        scheduler.step(val_metrics['macro_f1'])\n",
    "        \n",
    "        if val_metrics['macro_f1'] > best_f1:\n",
    "            best_f1 = val_metrics['macro_f1']\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "            print(f\"  New best model saved with Macro F1: {best_f1:.4f}\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {patience} epochs without improvement\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(\"best_model.pt\", weights_only=True))\n",
    "    \n",
    "    # Threshold tuning on validation set\n",
    "    val_metrics = evaluate_model(model, val_loader, techniques)\n",
    "    val_probs = val_metrics['probs']\n",
    "    val_labels = val_metrics['labels']\n",
    "    \n",
    "    thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "    best_thresholds = []\n",
    "    for i in range(num_classes):\n",
    "        class_f1 = []\n",
    "        for t in thresholds:\n",
    "            preds = (val_probs[:, i] > t).astype(int)\n",
    "            f1 = f1_score(val_labels[:, i], preds, zero_division=0)\n",
    "            class_f1.append(f1)\n",
    "        best_t = thresholds[np.argmax(class_f1)]\n",
    "        best_thresholds.append(best_t)\n",
    "        print(f\"Best threshold for {techniques[i]}: {best_t}\")\n",
    "    \n",
    "    return model, best_thresholds\n",
    "\n",
    "# Updated evaluation function to return probabilities for threshold tuning\n",
    "def evaluate_model(model, dataloader, techniques, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    total_loss = 0.0\n",
    "    criterion = FocalLoss(gamma=3, class_weights=class_weights)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "            preds = (probs > threshold).astype(float)\n",
    "            all_probs.extend(probs)\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    report = classification_report(all_labels, all_preds, target_names=techniques, zero_division=0)\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    return {'loss': total_loss / len(dataloader), 'macro_f1': macro_f1, 'report': report, 'probs': all_probs, 'labels': all_labels}\n",
    "\n",
    "# Run training and get best thresholds\n",
    "model, best_thresholds = train_model(model, train_loader, val_loader, techniques, num_epochs=6, patience=2)\n",
    "\n",
    "# Verify setup\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T08:11:12.101395Z",
     "iopub.status.busy": "2025-04-14T08:11:12.100828Z",
     "iopub.status.idle": "2025-04-14T08:13:40.875955Z",
     "shell.execute_reply": "2025-04-14T08:13:40.875088Z",
     "shell.execute_reply.started": "2025-04-14T08:11:12.101374Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 717/717 [02:28<00:00,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation Metrics:\n",
      "Macro F1: 0.3490\n",
      "\n",
      "Classification Report:\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "           loaded_language       0.70      0.80      0.74      2959\n",
      "   glittering_generalities       0.58      0.56      0.57       723\n",
      "                  euphoria       0.46      0.51      0.48       695\n",
      "            appeal_to_fear       0.27      0.52      0.35       449\n",
      "                       fud       0.35      0.53      0.43       576\n",
      "                 bandwagon       0.12      0.28      0.17       236\n",
      "thought_terminating_cliche       0.00      0.00      0.00       695\n",
      "              whataboutism       0.10      0.49      0.17       235\n",
      "            cherry_picking       0.28      0.66      0.40       768\n",
      "                 straw_man       0.13      0.29      0.18       207\n",
      "\n",
      "                 micro avg       0.42      0.58      0.49      7543\n",
      "                 macro avg       0.30      0.46      0.35      7543\n",
      "              weighted avg       0.45      0.58      0.50      7543\n",
      "               samples avg       0.33      0.41      0.34      7543\n",
      "\n",
      "\n",
      "Predicted Label Counts for Test Set:\n",
      "{'loaded_language': 3401, 'glittering_generalities': 701, 'euphoria': 784, 'appeal_to_fear': 874, 'fud': 862, 'bandwagon': 527, 'thought_terminating_cliche': 0, 'whataboutism': 1137, 'cherry_picking': 1779, 'straw_man': 447}\n",
      "\n",
      "Submission Shape: (5735, 11)\n",
      "\n",
      "Submission Head:\n",
      "                                      id  straw_man  appeal_to_fear  fud  \\\n",
      "0  521cd2e8-dd9f-42c4-98ba-c0c8890ff1ba          0               0    0   \n",
      "1  9b2a61e4-d14e-4ff7-b304-e73d720319bf          1               0    0   \n",
      "2  f0f1c236-80a8-4d25-b30c-a420a39be632          0               0    0   \n",
      "3  31ea05ba-2c2b-4b84-aba7-f3cf6841b204          0               0    0   \n",
      "4  a79e13ec-6d9a-40b5-b54c-7f4f743a7525          0               0    0   \n",
      "\n",
      "   bandwagon  whataboutism  loaded_language  glittering_generalities  \\\n",
      "0          0             0                1                        0   \n",
      "1          0             0                1                        0   \n",
      "2          0             0                1                        0   \n",
      "3          0             0                0                        0   \n",
      "4          1             0                1                        0   \n",
      "\n",
      "   euphoria  cherry_picking  cliche  \n",
      "0         0               0       0  \n",
      "1         0               1       0  \n",
      "2         0               0       0  \n",
      "3         0               0       0  \n",
      "4         0               1       0  \n",
      "\n",
      "Final submission saved as 'submission_with_labels.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load ground truth labels from solution.csv\n",
    "ground_truth_df = pd.read_csv('/kaggle/input/sample/solution.csv')\n",
    "\n",
    "# Define techniques as used in the model\n",
    "techniques = [\n",
    "    'loaded_language', 'glittering_generalities', 'euphoria', 'appeal_to_fear', \n",
    "    'fud', 'bandwagon', 'thought_terminating_cliche', 'whataboutism', \n",
    "    'cherry_picking', 'straw_man'\n",
    "]\n",
    "\n",
    "# Define the column order in the ground truth CSV (excluding 'Usage')\n",
    "gt_columns = [\n",
    "    'id', 'straw_man', 'appeal_to_fear', 'fud', 'bandwagon', 'whataboutism',\n",
    "    'loaded_language', 'glittering_generalities', 'euphoria', 'cherry_picking', 'cliche'\n",
    "]\n",
    "\n",
    "# Verify that ground truth CSV has the expected columns\n",
    "assert all(col in ground_truth_df.columns for col in gt_columns), \"Ground truth CSV missing some columns\"\n",
    "\n",
    "# Map ground truth columns to techniques (rename 'cliche' to 'thought_terminating_cliche')\n",
    "column_mapping = {\n",
    "    'straw_man': 'straw_man',\n",
    "    'appeal_to_fear': 'appeal_to_fear',\n",
    "    'fud': 'fud',\n",
    "    'bandwagon': 'bandwagon',\n",
    "    'whataboutism': 'whataboutism',\n",
    "    'loaded_language': 'loaded_language',\n",
    "    'glittering_generalities': 'glittering_generalities',\n",
    "    'euphoria': 'euphoria',\n",
    "    'cherry_picking': 'cherry_picking',\n",
    "    'cliche': 'thought_terminating_cliche'\n",
    "}\n",
    "\n",
    "# Ensure the ground truth CSV has the same order of IDs as test_df\n",
    "# If 'Usage' indicates a split, we can filter for 'Public' or use all rows\n",
    "# For now, use all rows to match test_df IDs\n",
    "ground_truth_df = ground_truth_df.set_index('id').loc[test_df['id']].reset_index()\n",
    "\n",
    "# Create a DataFrame with techniques in the model's order\n",
    "ground_truth_labels = ground_truth_df.rename(columns=column_mapping)[techniques].values.astype(int)\n",
    "\n",
    "# Create test dataset and dataloader\n",
    "test_dataset = TextDataset(test_df, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model_with_labels(model, dataloader, techniques, best_thresholds, true_labels):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_ids = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "            \n",
    "            # Apply per-class thresholds\n",
    "            preds = np.array([probs[:, i] > best_thresholds[i] for i in range(probs.shape[1])]).T.astype(int)\n",
    "            \n",
    "            all_probs.extend(probs)\n",
    "            all_preds.extend(preds)\n",
    "            \n",
    "            # Collect IDs\n",
    "            if \"id\" in batch:\n",
    "                all_ids.extend(batch[\"id\"].numpy().tolist())\n",
    "            else:\n",
    "                all_ids.extend(test_df[\"id\"].iloc[len(all_preds)-len(probs):len(all_preds)].tolist())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "    true_labels = np.array(true_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    report = classification_report(true_labels, all_preds, target_names=techniques, zero_division=0)\n",
    "    macro_f1 = f1_score(true_labels, all_preds, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'preds': all_preds,\n",
    "        'probs': all_probs,\n",
    "        'ids': all_ids,\n",
    "        'macro_f1': macro_f1,\n",
    "        'report': report,\n",
    "        'labels': true_labels\n",
    "    }\n",
    "\n",
    "# Run evaluation\n",
    "eval_result = evaluate_model_with_labels(model, test_loader, techniques, best_thresholds, ground_truth_labels)\n",
    "\n",
    "# Extract predictions and IDs\n",
    "test_preds = eval_result['preds']\n",
    "test_ids = eval_result['ids']\n",
    "\n",
    "# Verify prediction count\n",
    "if len(test_preds) != 5735:\n",
    "    raise ValueError(f\"Expected 5735 predictions, but got {len(test_preds)}\")\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nTest Set Evaluation Metrics:\")\n",
    "print(f\"Macro F1: {eval_result['macro_f1']:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", eval_result['report'])\n",
    "\n",
    "# Print predicted label distribution\n",
    "pred_counts = {tech: int(sum(test_preds[:, i])) for i, tech in enumerate(techniques)}\n",
    "print(\"\\nPredicted Label Counts for Test Set:\")\n",
    "print(pred_counts)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame(test_preds, columns=techniques)\n",
    "submission_df.insert(0, \"id\", test_ids)\n",
    "\n",
    "# Reorder columns and rename for submission\n",
    "submission_columns = [\n",
    "    \"id\", \"straw_man\", \"appeal_to_fear\", \"fud\", \"bandwagon\", \"whataboutism\",\n",
    "    \"loaded_language\", \"glittering_generalities\", \"euphoria\", \"cherry_picking\", \"cliche\"\n",
    "]\n",
    "submission_df = submission_df.rename(columns={'thought_terminating_cliche': 'cliche'})\n",
    "submission_df = submission_df[submission_columns]\n",
    "\n",
    "# Verify submission\n",
    "print(\"\\nSubmission Shape:\", submission_df.shape)\n",
    "if submission_df.shape[0] != 5735:\n",
    "    raise ValueError(f\"Submission should have 5735 rows, but found {submission_df.shape[0]}\")\n",
    "\n",
    "print(\"\\nSubmission Head:\\n\", submission_df.head())\n",
    "\n",
    "# Save submission\n",
    "submission_df.to_csv(\"submission_with_labels.csv\", index=False)\n",
    "print(\"\\nFinal submission saved as 'submission_with_labels.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6941088,
     "sourceId": 11129496,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6941580,
     "sourceId": 11130166,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6944597,
     "sourceId": 11134522,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6966696,
     "sourceId": 11164205,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6967398,
     "sourceId": 11165160,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6974437,
     "sourceId": 11174851,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6976523,
     "sourceId": 11177617,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6999317,
     "sourceId": 11209268,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7120418,
     "sourceId": 11373791,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6976510,
     "sourceId": 11378961,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
